use crate::domain::entities::app_result::{AppError, AppResult};
use crate::domain::entities::config::log_conf::{Log, Logger};
use crate::domain::entities::config::performance::Performance;
use crate::infrastructure::entities::vision::performance::ThreadPoolManager;

/// CPUäº²å’Œæ€§ç¤ºä¾‹ - å±•ç¤ºæ­£ç¡®çš„CPUæ ¸å¿ƒåˆ†é…
pub struct CpuAffinityExample;

impl CpuAffinityExample {
    /// æ¼”ç¤ºå¤šè®¾å¤‡CPUæ ¸å¿ƒåˆ†é…
    pub async fn demonstrate_cpu_allocation() -> AppResult<()> {
        Log::info("=== CPUæ ¸å¿ƒåˆ†é…æ¼”ç¤ºå¼€å§‹ ===");
        
        // å‡è®¾ç³»ç»Ÿæœ‰8ä¸ªCPUæ ¸å¿ƒï¼Œæ¯ä¸ªè®¾å¤‡åˆ†é…4ä¸ªæ ¸å¿ƒï¼Œæœ€å¤š2ä¸ªè®¾å¤‡
        let performance_config = Performance {
            cores_per_device: 4,
            max_devices: 2,
        };
        
        Log::info(&format!("ç³»ç»ŸCPUæ ¸å¿ƒæ•°: {}", num_cpus::get()));
        Log::info(&format!("é…ç½®: æ¯è®¾å¤‡{}æ ¸å¿ƒ, æœ€å¤§{}è®¾å¤‡", 
                         performance_config.cores_per_device, 
                         performance_config.max_devices));
        
        // åˆ›å»ºå¤šä¸ªè®¾å¤‡çš„çº¿ç¨‹æ± ç®¡ç†å™¨
        let mut managers = Vec::new();
        
        for device_id in 0..performance_config.max_devices {
            let manager = ThreadPoolManager::new_for_device(
                performance_config.clone(), 
                device_id
            )?;
            
            Log::info(&format!("âœ… {}", manager.display_cpu_allocation()));
            managers.push(manager);
        }
        
        // éªŒè¯CPUåˆ†é…ä¸é‡å 
        Self::verify_no_overlap(&managers)?;
        
        // æ¼”ç¤ºå¹¶å‘ä»»åŠ¡æ‰§è¡Œ
        Self::demonstrate_concurrent_execution(managers).await?;
        
        Log::info("=== CPUæ ¸å¿ƒåˆ†é…æ¼”ç¤ºå®Œæˆ ===");
        Ok(())
    }
    
    /// éªŒè¯CPUæ ¸å¿ƒåˆ†é…æ²¡æœ‰é‡å 
    fn verify_no_overlap(managers: &[ThreadPoolManager]) -> AppResult<()> {
        Log::info("ğŸ” éªŒè¯CPUæ ¸å¿ƒåˆ†é…æ˜¯å¦é‡å ...");
        
        let mut all_inference_cores = std::collections::HashSet::new();
        let mut all_cpu_cores = std::collections::HashSet::new();
        
        for manager in managers {
            let allocation = manager.cpu_allocation();
            
            // æ£€æŸ¥æ¨ç†æ ¸å¿ƒ
            if let Some(inference_core) = allocation.inference_core {
                if !all_inference_cores.insert(inference_core) {
                    return Err(AppError::ConfigError(format!(
                        "âŒ æ£€æµ‹åˆ°æ¨ç†æ ¸å¿ƒ{}é‡å ! è®¾å¤‡{}", 
                        inference_core, 
                        manager.device_id()
                    )));
                }
            }
            
            // æ£€æŸ¥CPUå¤„ç†æ ¸å¿ƒ
            for &cpu_core in &allocation.cpu_cores {
                if !all_cpu_cores.insert(cpu_core) {
                    return Err(AppError::ConfigError(format!(
                        "âŒ æ£€æµ‹åˆ°CPUå¤„ç†æ ¸å¿ƒ{}é‡å ! è®¾å¤‡{}", 
                        cpu_core, 
                        manager.device_id()
                    )));
                }
            }
        }
        
        Log::info(&format!("âœ… æ ¸å¿ƒåˆ†é…éªŒè¯é€šè¿‡! æ¨ç†æ ¸å¿ƒ: {:?}, CPUå¤„ç†æ ¸å¿ƒ: {:?}", 
                         all_inference_cores, all_cpu_cores));
        Ok(())
    }
    
    /// æ¼”ç¤ºå¹¶å‘æ‰§è¡Œ - æ¯ä¸ªè®¾å¤‡ä½¿ç”¨ä¸“ç”¨æ ¸å¿ƒ
    async fn demonstrate_concurrent_execution(managers: Vec<ThreadPoolManager>) -> AppResult<()> {
        Log::info("ğŸš€ å¼€å§‹å¹¶å‘æ‰§è¡Œæ¼”ç¤º...");
        
        let mut handles = Vec::new();
        
        for manager in managers {
            let handle = tokio::spawn(async move {
                Self::device_workload(manager).await
            });
            handles.push(handle);
        }
        
        // ç­‰å¾…æ‰€æœ‰è®¾å¤‡å®Œæˆ
        for handle in handles {
            handle.await
                .map_err(|e| AppError::InternalError(format!("è®¾å¤‡ä»»åŠ¡å¤±è´¥: {}", e)))??;
        }
        
        Log::info("âœ… æ‰€æœ‰è®¾å¤‡å¹¶å‘æ‰§è¡Œå®Œæˆ");
        Ok(())
    }
    
    /// å•ä¸ªè®¾å¤‡çš„å·¥ä½œè´Ÿè½½
    async fn device_workload(manager: ThreadPoolManager) -> AppResult<()> {
        let device_id = manager.device_id();
        Log::info(&format!("è®¾å¤‡{} å¼€å§‹å·¥ä½œè´Ÿè½½", device_id));
        
        // 1. æ¨ç†ä»»åŠ¡ï¼ˆä½¿ç”¨æ¨ç†ä¸“ç”¨æ ¸å¿ƒï¼‰
        let inference_pool = manager.inference_pool().clone();
        let inference_result = tokio::task::spawn_blocking(move || {
            inference_pool.install(|| {
                // æ¨¡æ‹Ÿæ¨ç†è®¡ç®—
                let mut sum = 0;
                for i in 0..1000000 {
                    sum += i * i;
                }
                sum
            })
        }).await.map_err(|e| AppError::InternalError(format!("æ¨ç†ä»»åŠ¡å¤±è´¥: {}", e)))?;
        
        Log::info(&format!("è®¾å¤‡{} æ¨ç†å®Œæˆ: {}", device_id, inference_result));
        
        // 2. CPUå¯†é›†å‹ä»»åŠ¡ï¼ˆä½¿ç”¨CPUå¤„ç†ä¸“ç”¨æ ¸å¿ƒï¼‰
        let cpu_pool = manager.cpu_pool().clone();
        let cpu_result = tokio::task::spawn_blocking(move || {
            cpu_pool.install(|| {
                use rayon::prelude::*;
                
                // å¹¶è¡ŒCPUå¯†é›†å‹è®¡ç®—ï¼ˆå¦‚CTCè§£ç ã€å›¾åƒå¤„ç†ç­‰ï¼‰
                (0..1000).into_par_iter().map(|i| {
                    // æ¨¡æ‹ŸCPUå¯†é›†å‹æ“ä½œ
                    std::thread::sleep(std::time::Duration::from_micros(1));
                    i * 2
                }).sum::<i32>()
            })
        }).await.map_err(|e| AppError::InternalError(format!("CPUä»»åŠ¡å¤±è´¥: {}", e)))?;
        
        Log::info(&format!("è®¾å¤‡{} CPUå¤„ç†å®Œæˆ: {}", device_id, cpu_result));
        
        // 3. æ˜¾ç¤ºæ ¸å¿ƒä½¿ç”¨æƒ…å†µ
        Log::info(&format!("è®¾å¤‡{} æ ¸å¿ƒä½¿ç”¨: {}", device_id, manager.display_cpu_allocation()));
        
        Ok(())
    }
    
    /// æ˜¾ç¤ºç†æƒ³çš„CPUåˆ†é…æ–¹æ¡ˆ
    pub fn show_ideal_allocation_example() {
        Log::info("=== ç†æƒ³CPUåˆ†é…æ–¹æ¡ˆç¤ºä¾‹ ===");
        
        // ç¤ºä¾‹1: 8æ ¸å¿ƒç³»ç»Ÿï¼Œ2ä¸ªè®¾å¤‡ï¼Œæ¯è®¾å¤‡4æ ¸å¿ƒ
        println!("ğŸ’¡ ç¤ºä¾‹1: 8æ ¸å¿ƒç³»ç»Ÿï¼Œ2ä¸ªè®¾å¤‡ï¼Œæ¯è®¾å¤‡4æ ¸å¿ƒ");
        println!("è®¾å¤‡0: æ¨ç†æ ¸å¿ƒ=0, CPUå¤„ç†æ ¸å¿ƒ=[1,2,3]");
        println!("è®¾å¤‡1: æ¨ç†æ ¸å¿ƒ=4, CPUå¤„ç†æ ¸å¿ƒ=[5,6,7]");
        println!("âœ… å®Œå…¨éš”ç¦»ï¼Œæ— ç«äº‰\n");
        
        // ç¤ºä¾‹2: 16æ ¸å¿ƒç³»ç»Ÿï¼Œ4ä¸ªè®¾å¤‡ï¼Œæ¯è®¾å¤‡4æ ¸å¿ƒ
        println!("ğŸ’¡ ç¤ºä¾‹2: 16æ ¸å¿ƒç³»ç»Ÿï¼Œ4ä¸ªè®¾å¤‡ï¼Œæ¯è®¾å¤‡4æ ¸å¿ƒ");
        println!("è®¾å¤‡0: æ¨ç†æ ¸å¿ƒ=0,  CPUå¤„ç†æ ¸å¿ƒ=[1,2,3]");
        println!("è®¾å¤‡1: æ¨ç†æ ¸å¿ƒ=4,  CPUå¤„ç†æ ¸å¿ƒ=[5,6,7]");
        println!("è®¾å¤‡2: æ¨ç†æ ¸å¿ƒ=8,  CPUå¤„ç†æ ¸å¿ƒ=[9,10,11]");
        println!("è®¾å¤‡3: æ¨ç†æ ¸å¿ƒ=12, CPUå¤„ç†æ ¸å¿ƒ=[13,14,15]");
        println!("âœ… å®Œå…¨éš”ç¦»ï¼Œå……åˆ†åˆ©ç”¨\n");
        
        // ç¤ºä¾‹3: 8æ ¸å¿ƒç³»ç»Ÿï¼Œ3ä¸ªè®¾å¤‡ï¼Œæ¯è®¾å¤‡4æ ¸å¿ƒï¼ˆè¶…é¢åˆ†é…ï¼‰
        println!("ğŸ’¡ ç¤ºä¾‹3: 8æ ¸å¿ƒç³»ç»Ÿï¼Œ3ä¸ªè®¾å¤‡ï¼Œæ¯è®¾å¤‡4æ ¸å¿ƒï¼ˆè¶…é¢åˆ†é…ï¼‰");
        println!("è®¾å¤‡0: æ¨ç†æ ¸å¿ƒ=0, CPUå¤„ç†æ ¸å¿ƒ=[1,2,3]");
        println!("è®¾å¤‡1: æ¨ç†æ ¸å¿ƒ=4, CPUå¤„ç†æ ¸å¿ƒ=[5,6,7]");
        println!("è®¾å¤‡2: æ¨ç†æ ¸å¿ƒ=0, CPUå¤„ç†æ ¸å¿ƒ=[1,2,3] (å–æ¨¡é‡ç”¨)");
        println!("âš ï¸  æœ‰é‡å ï¼Œä½†æ¯”éšæœºåˆ†é…å¥½\n");
        
        println!("ğŸ¯ å…³é”®ä¼˜åŠ¿:");
        println!("1. æ¨ç†çº¿ç¨‹å®Œå…¨éš”ç¦»ï¼Œæ€§èƒ½å¯é¢„æµ‹");
        println!("2. CPUå¤„ç†æ ¸å¿ƒä¸“ç”¨ï¼Œé¿å…ä¸Šä¸‹æ–‡åˆ‡æ¢");
        println!("3. æ•…éšœéš”ç¦»ï¼Œå•è®¾å¤‡é—®é¢˜ä¸å½±å“å…¶ä»–è®¾å¤‡");
        println!("4. NUMAæ„ŸçŸ¥ï¼ˆåœ¨NUMAç³»ç»Ÿä¸Šå¯è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰");
    }
}

/// CPUäº²å’Œæ€§å·¥å…·å‡½æ•°
pub mod cpu_affinity_utils {
    use super::*;

    /// è·å–å½“å‰çº¿ç¨‹çš„CPUäº²å’Œæ€§ï¼ˆéœ€è¦core_affinityä¾èµ–ï¼‰
    pub fn get_current_thread_affinity() -> Vec<usize> {
        // TODO: å®ç°è·å–å½“å‰çº¿ç¨‹CPUäº²å’Œæ€§
        // éœ€è¦ core_affinity = "0.8" ä¾èµ–
        /*
        if let Ok(core_ids) = core_affinity::get_current_thread_affinity() {
            core_ids.into_iter().map(|core| core.id).collect()
        } else {
            vec![]
        }
        */
        
        // ä¸´æ—¶è¿”å›ç©ºå‘é‡
        vec![]
    }
    
    /// éªŒè¯çº¿ç¨‹æ˜¯å¦ç»‘å®šåˆ°æ­£ç¡®çš„æ ¸å¿ƒ
    pub fn verify_thread_affinity(expected_core: usize) -> bool {
        let current_affinity = get_current_thread_affinity();
        current_affinity.contains(&expected_core)
    }
    
    /// è·å–ç³»ç»ŸNUMAæ‹“æ‰‘ä¿¡æ¯ï¼ˆé«˜çº§ä¼˜åŒ–ï¼‰
    pub fn get_numa_topology() -> AppResult<Vec<Vec<usize>>> {
        // TODO: å®ç°NUMAæ‹“æ‰‘æ£€æµ‹
        // åœ¨NUMAç³»ç»Ÿä¸Šï¼Œåº”è¯¥å°†è®¾å¤‡ç»‘å®šåˆ°åŒä¸€ä¸ªNUMAèŠ‚ç‚¹çš„æ ¸å¿ƒ
        
        // ä¸´æ—¶è¿”å›ç®€å•çš„åˆ†ç»„
        let total_cores = num_cpus::get();
        if total_cores >= 8 {
            // å‡è®¾å‰ä¸€åŠå’Œåä¸€åŠæ˜¯ä¸åŒçš„NUMAèŠ‚ç‚¹
            Ok(vec![
                (0..total_cores/2).collect(),
                (total_cores/2..total_cores).collect(),
            ])
        } else {
            Ok(vec![(0..total_cores).collect()])
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_cpu_allocation_no_overlap() {
        let performance_config = Performance {
            cores_per_device: 2,
            max_devices: 2,
        };
        
        let manager1 = ThreadPoolManager::new_for_device(performance_config.clone(), 0).unwrap();
        let manager2 = ThreadPoolManager::new_for_device(performance_config.clone(), 1).unwrap();
        
        let managers = vec![manager1, manager2];
        assert!(CpuAffinityExample::verify_no_overlap(&managers).is_ok());
    }
    
    #[test]
    fn test_cpu_allocation_calculation() {
        // æµ‹è¯•æ ¸å¿ƒåˆ†é…ç®—æ³•
        let total_cores = 8;
        
        // è®¾å¤‡0åº”è¯¥åˆ†é…æ ¸å¿ƒ0-3
        // è®¾å¤‡1åº”è¯¥åˆ†é…æ ¸å¿ƒ4-7
        
        // è¿™é‡Œå¯ä»¥æµ‹è¯•å…·ä½“çš„åˆ†é…é€»è¾‘
        assert!(true); // å ä½æµ‹è¯•
    }
}
