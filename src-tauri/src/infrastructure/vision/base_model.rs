use crate::infrastructure::logging::log_trait::Log;
use crate::infrastructure::ort::execution_provider_mgr::{
    configure_or_switch_provider, InferenceBackend,
};
use crate::infrastructure::vision::base_traits::ModelHandler;
use crate::infrastructure::vision::vision_error::{VisionError, VisionResult};
use memmap2::Mmap;
use ndarray::Array4;
use ort::inputs;
use ort::logging::LogLevel;
use ort::session::builder::GraphOptimizationLevel;
use ort::session::Session;
use ort::value::TensorRef;

/// åŸºç¡€æ¨¡å‹ç»“æ„ - åŒ…å«æ‰€æœ‰æ¨¡å‹çš„é€šç”¨å­—æ®µ
#[derive(Debug)]
pub struct BaseModel {
    pub session: Option<Session>,
    pub intra_thread_num: usize,
    pub intra_spinning: bool,
    pub inter_thread_num: usize,
    pub inter_spinning: bool,
    pub execution_provider: InferenceBackend,
    pub input_width: u32,
    pub input_height: u32,
    //pub model_path : Option<String>,
    pub model_bytes_map: Mmap,
    pub is_loaded: bool,
    pub model_type: ModelType,
}
#[derive(Debug)]
pub enum ModelType {
    Yolo11,
    PaddleDet5,
    PaddleCrnn5,
}

impl BaseModel {
    pub fn new(
        input_width: u32,
        input_height: u32,
        model_bytes_map: Mmap,
        execution_provider: InferenceBackend,
        intra_thread_num: usize,
        intra_spinning: bool,
        inter_thread_num: usize,
        inter_spinning: bool,
        model_type: ModelType,
    ) -> Self {
        Self {
            session: None,
            intra_thread_num,
            intra_spinning,
            inter_thread_num,
            inter_spinning,
            execution_provider,
            input_width,
            input_height,
            model_bytes_map,
            is_loaded: false,
            model_type,
        }
    }

    /// é€šç”¨çš„æ¨¡å‹åŠ è½½æ–¹æ³• - æ¶ˆé™¤é‡å¤ä»£ç 
    pub async fn load_model_base<T: ModelHandler>(
        &mut self,
        model_type_name: &str,
    ) -> VisionResult<()> {
        // 1. è§£ææ¨¡å‹è·¯å¾„
        /* let model_path = get_app_handle().await
        .path()
        .resolve(&self.model_path.clone().unwrap(), tauri::path::BaseDirectory::Resource)
        .map_err(|e| VisionError::LoadModelErr(format!("è§£æ{}æ¨¡å‹è·¯å¾„å¤±è´¥: {}", model_type_name, e)))?
        .to_string_lossy()
        .to_string();*/

        Log::info(&format!("åŠ è½½{}æ¨¡å‹", model_type_name));

        // 2. åˆ›å»ºsession builder
        let result = configure_or_switch_provider(None, "cuda").map_err(|e| {
            VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            }
        })?;

        let session_builder = result.builder;
        Log::info(&format!("å½“å‰ä½¿ç”¨æ‰§è¡Œå™¨: {}", result.active_backend.name()));

        // 4. åŠ è½½æ¨¡å‹æ–‡ä»¶
        let session = session_builder
            .with_optimization_level(GraphOptimizationLevel::Level3)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_intra_threads(self.intra_thread_num)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_log_level(LogLevel::Error)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_intra_op_spinning(self.intra_spinning)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_inter_threads(self.inter_thread_num)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_inter_op_spinning(self.inter_spinning)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .commit_from_memory_directly(&self.model_bytes_map.as_ref())
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?;
        //.commit_from_file(model_path)
        //.map_err(|e| VisionError::LoadModelErr(format!("åŠ è½½{}æ¨¡å‹æ–‡ä»¶å¤±è´¥: {}", model_type_name, e)))?;

        // 5. æ›´æ–°çŠ¶æ€
        //self.session = Some(Arc::new(Mutex::new(session)));
        self.session = Some(*session);
        self.is_loaded = true;

        Log::debug(&format!("{}æ¨¡å‹åŠ è½½æˆåŠŸ", model_type_name));
        Ok(())
    }

    /// é€šç”¨çš„æ¨ç†æ–¹æ³• - æ¶ˆé™¤æ¨ç†ä»£ç é‡å¤ ğŸ†•
    /// æ­£ç¡®ä½¿ç”¨ORTçº¿ç¨‹è®¾ç½®å’ŒRayonçº¿ç¨‹æ± é…åˆ
    pub async fn inference_base<T: ModelHandler>(
        &self,
        input: Array4<f32>,
        handler: &T,
    ) -> VisionResult<Array4<f32>> {
        if let Some(session) = &self.session {
            // å°è¯•è·å–æ¨ç†çº¿ç¨‹æ± ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°æ™®é€šæ¨ç†
            let mut session_guard = session.lock().await;

            // åˆ›å»ºè¾“å…¥å¼ é‡
            let input_tensor =
                TensorRef::from_array_view(&input).map_err(|e| VisionError::DataProcessingErr {
                    method: "inference_base".to_string(),
                    e: e.to_string(),
                })?;

            // æ‰§è¡Œæ¨ç† - ORTå†…éƒ¨ä½¿ç”¨å•çº¿ç¨‹(ç”±with_intra_threads(1)æ§åˆ¶)
            let outputs = session_guard
                .run(inputs![handler.get_input_node_name() => input_tensor])
                .map_err(|e| VisionError::InferenceErr {
                    method: "inference_base".to_string(),
                    e: e.to_string(),
                })?;

            // æå–è¾“å‡º
            let view = outputs[handler.get_output_node_name()]
                .try_extract_array::<f32>()
                .map_err(|e| VisionError::DataProcessingErr {
                    method: "inference_base".to_string(),
                    e: e.to_string(),
                })?;

            // å¤„ç†ä¸åŒçš„è¾“å‡ºæ ¼å¼
            let output = match self.model_type {
                // YOLOéœ€è¦è½¬ç½®
                ModelType::Yolo11 => view.t().into_owned(),
                ModelType::PaddleCrnn5 => view.into_owned(),
                ModelType::PaddleDet5 => view.into_owned(),
            };

            // é‡å¡‘è¾“å‡ºå½¢çŠ¶
            let shape: [usize; 4] = {
                let s = output.shape();
                [s[0], s[1], s[2], s[3]]
            };

            output
                .into_shape_with_order(ndarray::Ix4(shape[0], shape[1], shape[2], shape[3]))
                .map_err(|e| VisionError::DataProcessingErr {
                    method: "inference_base".to_string(),
                    e: e.to_string(),
                })?
        } else {
            Err(VisionError::IoError {
                path: "[æ¨ç†é˜¶æ®µ]".to_string(),
                e: "æ¨¡å‹æœªåŠ è½½",
            })
        }
    }
}
