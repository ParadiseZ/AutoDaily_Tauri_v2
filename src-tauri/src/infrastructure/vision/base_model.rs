use crate::domain::vision::result::{DetResult, OcrResult};
use crate::infrastructure::logging::log_trait::Log;
use crate::infrastructure::ort::execution_provider_mgr::{
    configure_or_switch_provider, InferenceBackend,
};
use crate::infrastructure::vision::base_traits::ModelHandler;
use crate::infrastructure::vision::vision_error::{VisionError, VisionResult};
use memmap2::Mmap;
use ndarray::{ArrayD, ArrayViewD};
use ort::inputs;
use ort::logging::LogLevel;
use ort::session::builder::GraphOptimizationLevel;
use ort::session::Session;
use ort::value::TensorRef;
use std::sync::Mutex;

/// åŸºç¡€æ¨¡å‹ç»“æ„ - åŒ…å«æ‰€æœ‰æ¨¡å‹çš„é€šç”¨å­—æ®µ

pub struct BaseModel {
    pub session: Option<Mutex<Session>>,
    pub intra_thread_num: usize,
    pub intra_spinning: bool,
    pub inter_thread_num: usize,
    pub inter_spinning: bool,
    pub execution_provider: InferenceBackend,
    pub input_width: u32,
    pub input_height: u32,
    //pub model_path : Option<String>,
    pub model_bytes_map: Mmap,
    pub is_loaded: bool,
    pub model_type: ModelType,
}

impl std::fmt::Debug for BaseModel{
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "BaseModel[session:hidden, intra_thread_num: {}, intra_spinning: {}, inter_thread_num: {}, inter_spinning: {}, execution_provider: {:?}, input_width: {}, input_height: {}, model_bytes_map: hidden, is_loaded: {}, model_type: {:?}]",
            self.intra_thread_num,
            self.intra_spinning,
            self.inter_thread_num,
            self.inter_spinning,
            self.execution_provider,
            self.input_width,
            self.input_height,
            self.is_loaded,
            self.model_type
        )
    }
}

#[derive(Debug)]
pub enum ModelType {
    Yolo11,
    PaddleDet5,
    PaddleCrnn5,
}

#[derive(Debug)]
pub enum PostprocessRes{
    Detection(Vec<DetResult>),
    Recognition(Vec<OcrResult>),
}

impl BaseModel {
    pub fn new(
        input_width: u32,
        input_height: u32,
        model_bytes_map: Mmap,
        execution_provider: InferenceBackend,
        intra_thread_num: usize,
        intra_spinning: bool,
        inter_thread_num: usize,
        inter_spinning: bool,
        model_type: ModelType,
    ) -> Self {
        Self {
            session: None,
            intra_thread_num,
            intra_spinning,
            inter_thread_num,
            inter_spinning,
            execution_provider,
            input_width,
            input_height,
            model_bytes_map,
            is_loaded: false,
            model_type,
        }
    }

    /// é€šç”¨çš„æ¨¡å‹åŠ è½½æ–¹æ³• - æ¶ˆé™¤é‡å¤ä»£ç 
    pub fn load_model_base<T: ModelHandler>(
        &mut self,
        model_type_name: &str,
    ) -> VisionResult<()> {
        // 1. è§£ææ¨¡å‹è·¯å¾„

        Log::info(&format!("åŠ è½½{}æ¨¡å‹", model_type_name));

        // 2. åˆ›å»ºsession builder
        let result = configure_or_switch_provider(None, "cuda").map_err(|e| {
            VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            }
        })?;

        let session_builder = result.builder;
        Log::info(&format!("å½“å‰ä½¿ç”¨æ‰§è¡Œå™¨: {}", result.active_backend.name()));

        // 4. åŠ è½½æ¨¡å‹æ–‡ä»¶
        let session = session_builder
            .with_optimization_level(GraphOptimizationLevel::Level3)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_intra_threads(self.intra_thread_num)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_log_level(LogLevel::Error)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_intra_op_spinning(self.intra_spinning)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_inter_threads(self.inter_thread_num)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .with_inter_op_spinning(self.inter_spinning)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?
            .commit_from_memory(&self.model_bytes_map)
            .map_err(|e| VisionError::SessionConfigFailed {
                method: "load_model_base".to_string(),
                e: e.to_string(),
            })?;


        // 5. æ›´æ–°çŠ¶æ€
    self.session = Some(Mutex::new(session));
    self.is_loaded = true;

        Log::debug(&format!("{}æ¨¡å‹åŠ è½½æˆåŠŸ", model_type_name));
        Ok(())
    }

    /// é€šç”¨çš„æ¨ç†æ–¹æ³• - æ¶ˆé™¤æ¨ç†ä»£ç é‡å¤ ğŸ†•
    /// æ­£ç¡®ä½¿ç”¨ORTçº¿ç¨‹è®¾ç½®å’ŒRayonçº¿ç¨‹æ± é…åˆ
    pub fn inference_base(
        &self,
        input: ArrayViewD<'_, f32>,
        input_node_name: &str,
        output_node_name: &str,
    ) -> VisionResult<ArrayD<f32>> {
        if let Some(session_mutex) = self.session.as_ref() {
            // åˆ›å»ºè¾“å…¥å¼ é‡
            let input_tensor =
                TensorRef::from_array_view(input).map_err(|e| VisionError::DataProcessingErr {
                    method: "inference_base".to_string(),
                    e: e.to_string(),
                })?;

            // è·å–é”
            let mut session = session_mutex.lock().map_err(|_| VisionError::InferenceErr {
                method: "inference_base".to_string(),
                e: "è·å–Sessioné”å¤±è´¥".to_string(),
            })?;

            // æ‰§è¡Œæ¨ç†
            let outputs = session
                .run(inputs![input_node_name => input_tensor])
                .map_err(|e| VisionError::InferenceErr {
                    method: "inference_base".to_string(),
                    e: e.to_string(),
                })?;

            // æå–è¾“å‡º
            let view = outputs[output_node_name]
                .try_extract_array::<f32>()
                .map_err(|e| VisionError::DataProcessingErr {
                    method: "inference_base".to_string(),
                    e: e.to_string(),
                })?;
            Log::debug(&format!("æ¨¡å‹è¾“å‡ºç»´åº¦: {}", view.ndim()));
            // å¤„ç†ä¸åŒçš„è¾“å‡ºæ ¼å¼
            let output = match self.model_type {
                // YOLOéœ€è¦è½¬ç½®
                ModelType::Yolo11 => view.t().to_owned(),
                ModelType::PaddleCrnn5 => view.to_owned(),
                ModelType::PaddleDet5 => view.to_owned(),
            };

            // ç›´æ¥è¿”å› ArrayDynï¼Œç”±è°ƒç”¨è€…å¤„ç†å…·ä½“çš„ç»´åº¦é€»è¾‘
            Ok(output)
        } else {
            Err(VisionError::IoError {
                path: "[æ¨ç†é˜¶æ®µ]".to_string(),
                e: "æ¨¡å‹æœªåŠ è½½".to_string(),
            })
        }
    }
}
